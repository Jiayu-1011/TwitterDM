{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于LDA主题模型和情感词典的Twitter推文主题提取及情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作环境：(jupyter notebook)\n",
    "\n",
    "\n",
    "python 3.7.5\n",
    "\n",
    "pandas 1.0.1 (处理表格数据)\n",
    "\n",
    "wordcloud 1.8.1 (词云)\n",
    "\n",
    "stylecloud 0.5.1 (词云)\n",
    "\n",
    "nltk 3.5 (内含各种停用词，以及词干提取，词形还原和分词的方法)\n",
    "\n",
    "gensim 3.8.3 (内含LDA模型)\n",
    "\n",
    "pyLDAvis 3.2.2 (LDA模型可视化工具)\n",
    "\n",
    "numpy 1.17.4 (数学工具)\n",
    "\n",
    "matplotlib 3.1.1 (数据可视化绘图工具)\n",
    "\n",
    "seaborn 0.11.0\n",
    "\n",
    "Pillow 7.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念说明\n",
    " \n",
    " \n",
    "#### 什么是LDA？\n",
    "\n",
    "> LDA（隐含狄利克雷分布）**将文档集中的每篇文档的主题以概率分布的形式给出**\n",
    "\n",
    "\n",
    "\n",
    "#### 什么是TF-IDF？\n",
    "\n",
    "> TF: 词频(term frequency) （对于某个文本，单词在某个文本中出现越多次越重要）\n",
    "\n",
    "> IDF: 逆文档频率(inverse document frequency) （单词在语料库（所有文本）中出现越少次越重要）\n",
    "\n",
    "> TF-IDF是一种衡量某一篇文档中某个词对该篇文档重要程度的计算方法。通过TF-IDF公式，我们可以计算出这个词对于表现这篇文档主题而言贡献如何。TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类或提取关键词。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 什么是情感词典？\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流程\n",
    "1. 数据获取:\n",
    "\n",
    "使用python爬虫爬取twitter数据,时间线(2020.3.12-2021.1.12)\n",
    "\n",
    "2. 数据清洗:\n",
    "\n",
    "    ① 清除存在推文或发布时间缺失的数据条目\n",
    "    \n",
    "    ② 将数据按照发布时间前后顺序排序\n",
    "    \n",
    "    ③ 数据分布初步分析（随发布时间）\n",
    "\n",
    "3. 文本预处理:\n",
    "\n",
    "    ① Twitter_data['content'] 初始数据(matrix size:)\n",
    "    \n",
    "    ② Processed_data 去除了标点、@、短词(matrix size:)\n",
    "    \n",
    "    ③ data_words 去除了nltk停用词词典中的停用词、自定义的禁用词列表（如搜索关键词）(matrix size:)\n",
    "\n",
    "\n",
    "4. **情感分析:**\n",
    "\n",
    "    ① 根据TF-IDF算法对推文进行关键词提取，进行词云展示（总体、逐月）\n",
    "    \n",
    "    ② 使用LDA主题模型进行训练，提取主题分布\n",
    "    \n",
    "        1. 构建LDA模型，进行训练\n",
    "\n",
    "        2. 计算模型困惑度(Perplexity)和模型一致性得分(Coherence Score)，其中困惑度越低越好，一致性得分越高越好\n",
    "\n",
    "        3. 为每篇推文分配对应占主导地位的主题（概率较高的）\n",
    "\n",
    "        4. 调整主题选取数或训练迭代次数等其他参数，迭代调优\n",
    "        \n",
    "    ③ 根据情感词典SentiWordNet3.0进行文本得分计算，每篇推文会计算出一个得分。其情感越积极，则得分越高；反之情感越消极，则得分越低\n",
    " \n",
    " \n",
    "\n",
    "5. 结果分析\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 优点：\n",
    "- 相比于使用机器学习和深度学习的方法，使用情感词典计算文本得分，无需经过模型的训练和调参，过程更为直观，也更适合于像twitter推文这种短文本的分析（短文本通常有很多逻辑不完整且不符合语法规则的句子）\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 局限性：\n",
    "- 情感词典方面，单词得分直接由相关单词得出，并未考虑程度副词的影响\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import stylecloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化参数设置\n",
    "- 全局选取关键词Top N\n",
    "- 逐月选取关键词Top N\n",
    "- 主题数选取\n",
    "- 训练迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 全局选取关键词Top N\n",
    "GLOBAL_TOP_N = 20\n",
    "\n",
    "# 逐月选取关键词Top N\n",
    "PER_MONTH_TOP_N = 10\n",
    "\n",
    "# 主题数选取\n",
    "NUM_TOPICS = 4\n",
    "\n",
    "# 训练迭代次数\n",
    "ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 指定当前工作路径\n",
    "# print(os.getcwd())\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "# 从csv文件中读入数据\n",
    "Twitter_data = pd.read_excel('./twitter_covid.xlsx')\n",
    "\n",
    "# 初始数据部分展示\n",
    "Twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 规范化数据 列名称\n",
    "Twitter_data.rename(\n",
    "    columns={'m_content': 'content', 'g_publish_time': 'publish_time', 'm_content_id': 'content_id'},\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 爬取初始数据存在空行，需要进行处理\n",
    "# 可以看到有11744条非空推文，却有11841条发布日期，所以数据肯定存在部分缺失\n",
    "Twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 清除缺失推文或缺失发布时间的数据\n",
    "Twitter_data.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 清除了推文为空的数据项\n",
    "Twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 将发布时间转换成python中的datetime格式,便于后续操作,并将发布时间统一为 year-month-date 的格式\n",
    "\n",
    "# PS:不使用pd.to_datetime是因为对于某一条数据，我们只关心其发布的日期，在一天中的具体时间并不关心，\n",
    "# 而pd.datetime会带上 hour-min-sec ，不利于后续操作\n",
    "Twitter_data['publish_time'] = Twitter_data['publish_time'].map \\\n",
    "(lambda x: datetime.date((int)(x.split('-')[0]), (int)(x.split('-')[1]), (int)(x.split('-')[2].split(' ')[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 所有数据按照发布时间排序\n",
    "Twitter_data = Twitter_data.sort_values(by='publish_time')\n",
    "Twitter_data.reset_index(inplace=True)\n",
    "del Twitter_data['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 观察到不在预期时间范围内的数据，进行清除\n",
    "Twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#设定起始、截止日期\n",
    "start_date = datetime.date(2020, 3, 12)\n",
    "end_date = datetime.date(2021, 1, 12)\n",
    "\n",
    "\n",
    "for index, date in enumerate(Twitter_data['publish_time']):\n",
    "    # 在起始日期前或截止日期后,为不符合要求的数据\n",
    "    if date.__sub__(start_date).days<0 or date.__sub__(end_date).days>0:\n",
    "        Twitter_data.drop(index=index, inplace=True)\n",
    "        \n",
    "Twitter_data = Twitter_data.reset_index()\n",
    "del Twitter_data['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 可以观察到数据开始和结束都已经符合预期\n",
    "Twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Twitter_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察原数据在2020.3.12-2021.1.12的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def format_month_or_day(month_or_day):\n",
    "    if int(month_or_day)>0 and int(month_or_day)<10:\n",
    "        return '0' + str(month_or_day)\n",
    "    else:\n",
    "        return month_or_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 将publish_time进行拆分便于后续统计\n",
    "Twitter_data['year'] = Twitter_data['publish_time'].map(lambda x: x.year)\n",
    "Twitter_data['month'] = Twitter_data['publish_time'].map(lambda x: x.month)\n",
    "Twitter_data['date'] = Twitter_data['publish_time'].map(lambda x: format_month_or_day(x.day))\n",
    "Twitter_data['year_month'] = Twitter_data['publish_time'].map(lambda x: str(x.year) + '-' + str(format_month_or_day(x.month)))\n",
    "Twitter_data['times'] = Twitter_data['publish_time'].map(lambda x: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "Twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 对于某一天的数据频次统计如下,times为频次\n",
    "day_data = Twitter_data.groupby(['publish_time'], as_index=False).agg({'times': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 对于某个月的数据频次统计如下，times为频次\n",
    "month_data = Twitter_data.groupby(['year_month', 'year', 'month'], as_index=False).agg({'times': 'sum'})\n",
    "month_data['day'] = 1\n",
    "month_data['publish_time'] = pd.to_datetime(month_data[['year', 'month', 'day']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "month_data[['year_month', 'times']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 数据量随日期分布的折线图，可以看到某些日期的数据量较少，总体来说相对均匀\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "fig.set_size_inches(16,5)\n",
    "\n",
    "plt.plot(day_data['publish_time'], day_data['times'], linewidth=1.3, label='Daily', color='orange')\n",
    "ax.set(xlabel='count',title='Distribution of tweets by date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 数据量随月份分布的柱状图，可以看到2020-3和2021-1的数据量较少，其他月份数据量都较均匀\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "fig.set_size_inches(16,5)\n",
    "rects = plt.bar \\\n",
    "(range(len(month_data['month'])), month_data['times'], tick_label=month_data['year_month'], color='rgb')\n",
    "\n",
    "# 打数据标注\n",
    "for rect in rects:  #rects 是三根柱子的集合\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width() / 2, height, str(height), size=13, ha='center', va='bottom')\n",
    "\n",
    "plt.title('Distribution of tweets by year-month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.hist(Twitter_data['publish_time'], density=True, bins='auto',align='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 数据量随月份分布的饼图\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "plt.pie(month_data['times'],labels=month_data['year_month'],autopct='%1.1f%%',shadow=False,startangle=150)\n",
    "plt.title('Distribution of tweets by year-month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据分布情况：2020-3和2021-1的数据量较少，其他月份数据量都较均匀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print(string.punctuation)\n",
    "\n",
    "def text_preprocess(data):\n",
    "    # 去除@user\n",
    "    processed_data = data.map(lambda x: re.sub(r'@[\\w]*', '', x))\n",
    "\n",
    "    # 去除网址\n",
    "    processed_data = processed_data.map(lambda x: re.sub(r'http.*\\..* ', '', x))\n",
    "\n",
    "    # 去除标点符号\n",
    "    processed_data = processed_data.map(lambda x: re.sub(r'[^a-zA-Z ]', '', x))\n",
    "\n",
    "    # 删除长度太短的词（通常无意义）\n",
    "    processed_data = processed_data.map(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "    # 分词\n",
    "    processed_data = processed_data.map(lambda x: nltk.WordPunctTokenizer().tokenize(x))\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 去除标点、@、短词，进行分词\n",
    "Processed_data = text_preprocess(Twitter_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 经过预处理后的单词矩阵\n",
    "Processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 查看停用词词典\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "# 去除停用词\n",
    "data_words = remove_stopwords(Processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 去除停用词后的单词矩阵\n",
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 去除搜索关键词\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 导入下面三种词干提取器进行对比\n",
    "import nltk.stem.porter as pt\n",
    "import nltk.stem.lancaster as lc\n",
    "import nltk.stem.snowball as sb\n",
    "\n",
    "# 导入nltk.stem用来词型还原\n",
    "import nltk.stem as ns\n",
    "\n",
    "def word_stem(text_matrix):\n",
    "\n",
    "    print(\"----------词干提取-------------\")\n",
    "    # 在名词和动词中，除了与数和时态有关的成分以外的核心成分。\n",
    "    # 词干并不一定是合法的单词\n",
    "\n",
    "    pt_stemmer = pt.PorterStemmer()  # 波特词干提取器\n",
    "    lc_stemmer = lc.LancasterStemmer()   # 兰卡斯词干提取器\n",
    "    sb_stemmer = sb.SnowballStemmer(\"english\")# 思诺博词干提取器\n",
    "\n",
    "    print(\"%8s %8s %8s %8s\" % ('word','pt_stem','lc_stem','sb_stem'))\n",
    "    for text in text_matrix:\n",
    "        for word in text:\n",
    "            pt_stem = pt_stemmer.stem(word)\n",
    "            lc_stem = lc_stemmer.stem(word)\n",
    "            sb_stem = sb_stemmer.stem(word)\n",
    "            print(\"%8s %8s %8s %8s\" % (word,pt_stem,lc_stem,sb_stem))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immunity    immun    immun    immun\n",
      " bidding      bid      bid      bid\n",
      " vaccine   vaccin   vaccin   vaccin\n",
      "    done     done      don     done\n",
      "guyreporter guyreport guyreport guyreport\n",
      "   wants     want     want     want\n",
      "   argue     argu     argu     argu\n",
      "   trump    trump    trump    trump\n",
      "    hope     hope      hop     hope\n",
      " working     work     work     work\n",
      "   hates     hate      hat     hate\n",
      "   trump    trump    trump    trump\n",
      "something   someth   someth   someth\n",
      "    cure     cure      cur     cure\n",
      "   covid    covid    covid    covid\n",
      "   covid    covid    covid    covid\n",
      " vaccine   vaccin   vaccin   vaccin\n",
      "  donald   donald   donald   donald\n",
      "   trump    trump    trump    trump\n",
      "    call     call      cal     call\n",
      "    fake     fake      fak     fake\n",
      "    news     news      new     news\n"
     ]
    }
   ],
   "source": [
    "# 展示一下词干提取的结果，但实际并没有使用词干提取，因为粒度过粗\n",
    "word_stem(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 词型还原：复数名词->单数名词 ；分词->动词原型\n",
    "# 单词原型一定是合法的单词\n",
    "\n",
    "def word_lemma(text_matrix): \n",
    "    print(\"----------词型还原器---------------\")\n",
    "    \n",
    "    print(\"%8s %8s %8s\" % ('word','n_lemma','v_lemma'))\n",
    "    lemmatizer = ns.WordNetLemmatizer()\n",
    "    for index_txt, text in enumerate(text_matrix):\n",
    "        for index_word, word in enumerate(text):\n",
    "            # 将名词还原为单数形式\n",
    "            n_lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "            # 将动词还原为原型形式\n",
    "            v_lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            print('%8s %8s %8s' % (word, n_lemma, v_lemma))\n",
    "            \n",
    "            if(len(n_lemma)<len(v_lemma)):\n",
    "                text_matrix[index_txt][index_word] = n_lemma\n",
    "            else:\n",
    "                text_matrix[index_txt][index_word] = v_lemma\n",
    "                \n",
    "    return text_matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 实际使用了粒度更细的词形还原\n",
    "data_words = word_lemma(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 经过词形还原后的单词矩阵\n",
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 去除搜索关键词相关的单词\n",
    "ban_words_list = []\n",
    "with open('./ban_words_list.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        ban_words_list.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ban_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_search_words(words_matrix, ban_words_list):\n",
    "    words_processed = []\n",
    "    for text in words_matrix:\n",
    "        words_text = []\n",
    "        for word in text:\n",
    "            if word not in ban_words_list:\n",
    "                words_text.append(word)\n",
    "                \n",
    "        words_processed.append(words_text)\n",
    "        \n",
    "    return words_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_words = remove_search_words(data_words, ban_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF（关键词权重）与LDA（词分布）的区别\n",
    "\n",
    "> 使用TF-IDF进行关键词提取的局限性在于: 在某些场景，基于文档本身的关键词提取还不是非常足够，有些关键词并不一定会显示地出现在文档当中，如一篇讲动物生存环境的科普文，通篇介绍狮子老虎等，但是文中并没有显示地出现动物二字。\n",
    "\n",
    "> 而LDA主题模型认为在词与文档之间没有直接的联系，它们应当还有一个维度将它们串联起来，主题模型将这个维度称为主题。每个文档都应该对应着一个或多个的主题，而每个主题都会有对应的词分布，通过主题，就可以得到每个文档的词分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于TF-IDF算法生成词云\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型(bag of words)\n",
    "- 将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息\n",
    "- 词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 构造词典（将单词词典化，编号和单词具有一一对应关系）\n",
    "dictionary = corpora.Dictionary(data_words)\n",
    "\n",
    "texts = data_words\n",
    "\n",
    "# 统计词频，构造词频矩阵（词袋）\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 构造TF-IDF模型\n",
    "tf_idf_model = TfidfModel(corpus, normalize=True)\n",
    "word_tf_idf = list(tf_idf_model[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 词典\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 词频（每一行代表一篇推文，某一行中的某个元组对（x,y）代表编号为x的单词在该推文中出现了y次）\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 每个句子对应的词袋 (前面是单词，后面是词频)\n",
    "# 词频矩阵反向化（id->单词）\n",
    "\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in corpus]\n",
    "print(data_words[1])\n",
    "print(id_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF得分，每篇推文中每个单词都有对应的TF_IDF得分，该得分已经进行了标准化，可以视作一个单词的权重\n",
    "word_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 基于TF-IDF算法提取关键词\n",
    "def TF_IDF_get_keywords(words_matrix, top_n=3):\n",
    "    # 构建词袋模型\n",
    "\n",
    "    # 构造词典（将单词词典化，编号和单词具有一一对应关系）\n",
    "    dictionary = corpora.Dictionary(words_matrix)\n",
    "\n",
    "    texts = words_matrix\n",
    "\n",
    "    # 统计词频，构造词频矩阵（词袋）\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    tf_idf_model = TfidfModel(corpus, normalize=True)\n",
    "    word_tf_idf = list(tf_idf_model[corpus])\n",
    "    \n",
    "    key_words_matrix = []\n",
    "    \n",
    "    for index_txt, text in enumerate(word_tf_idf):\n",
    "        print('tweet %d \\'s keywords: ' % index_txt)\n",
    "        text.sort(key=lambda x: x[1], reverse=True)\n",
    "        key_words_code = [word[0] for word in text[:top_n+3]]\n",
    "        key_words = []\n",
    "#         print(key_words_code)\n",
    "        for code in key_words_code:\n",
    "            if dictionary[code] not in ban_words_list:\n",
    "                key_words.append(dictionary[code])\n",
    "            if len(key_words)>=3:\n",
    "                break\n",
    "        \n",
    "        print(key_words)\n",
    "        print()\n",
    "            \n",
    "        key_words_matrix.append(key_words)\n",
    "    \n",
    "    return key_words_matrix\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 提取每篇推文Top3的TF-IDF得分的关键词\n",
    "key_words_global = TF_IDF_get_keywords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "key_words_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 提取高频关键词\n",
    "def get_highest_frequency_words(words, top_n=10):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word in frequency:\n",
    "            frequency[word] += 1\n",
    "        else:\n",
    "            frequency[word] = 1\n",
    "    \n",
    "#     print(frequency)\n",
    "    top_words = sorted(frequency.items(), key=lambda x: x[1],reverse=True)[:top_n]\n",
    "    \n",
    "#     print(top_words)\n",
    "    top_words_dict = {}\n",
    "    for word in top_words:\n",
    "        top_words_dict[word[0]] = word[1]\n",
    "        \n",
    "        \n",
    "    return top_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 提取每个月出现频率最高的关键词\n",
    "key_words_global_general = {}\n",
    "\n",
    "key_words = []\n",
    "for text in key_words_global:\n",
    "    for word in text:\n",
    "        key_words.append(word)\n",
    "    \n",
    "# print(key_words)\n",
    "\n",
    "key_words_global_general = get_highest_frequency_words(key_words, top_n=GLOBAL_TOP_N)\n",
    "\n",
    "# 将Top20关键词输出至./keywords/global/Top20_keywords_global.txt文件中\n",
    "if not os.path.exists('./key_words/global'):\n",
    "    os.makedirs('./key_words/global')\n",
    "if not os.path.exists('./key_words/global/Top20_keywords_global.txt'):\n",
    "\n",
    "    with open('./key_words/global/Top20_keywords_global.txt', 'w') as f:\n",
    "        for key_word in key_words_global_general:\n",
    "            f.write(key_word+' '+str(key_words_global_general[key_word])+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "key_words_global_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 生成全局词云\n",
    "\n",
    "# 连接全局关键词矩阵中所有单词\n",
    "global_string =  ' '.join([' '.join(word) for word in key_words_global])\n",
    "\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud \\\n",
    "# (height=400, width=800, background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# \n",
    "# wordcloud.generate(global_string)\n",
    "# wordcloud.to_image()\n",
    "# if not os.path.exists('./word_cloud/global/wordcloud_global.png'):\n",
    "#     wordcloud.to_file('./word_cloud/global/wordcloud_global.png')\n",
    "\n",
    "\n",
    "if not os.path.exists('./word_cloud/global'):\n",
    "    os.makedirs('./word_cloud/global')\n",
    "\n",
    "# 生成词云    \n",
    "    \n",
    "if not os.path.exists('./word_cloud/global/wordcloud_global.png'):\n",
    "    \n",
    "    stylecloud.gen_stylecloud( \n",
    "        text=global_string, \n",
    "        icon_name=\"fab fa-twitter\", # 使用推特图标蒙版\n",
    "        gradient='horizontal', # 渐变色方向选了垂直方向\n",
    "        max_words=200,\n",
    "        output_name='./word_cloud/global/wordcloud_global.png',\n",
    "    )\n",
    "\n",
    "# 词云可视化\n",
    "\n",
    "img = Image.open('./word_cloud/global/wordcloud_global.png')\n",
    "    \n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('global wordcloud', fontsize='xx-large',fontweight='heavy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "month_data['times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 逐月分割关键词矩阵\n",
    "'''\n",
    "input: 单词矩阵（分词后的单词 / 提取后的关键词矩阵）, shape:\n",
    "       逐月数据条数（每个月有多少条数据，前面已经统计过了）, \n",
    "       具体月份（这里使用year-month）\n",
    "       \n",
    "output: 逐月分割后的单词矩阵（增加了一个维度） \n",
    "'''\n",
    "\n",
    "def words_separate_by_month(words_matrix, times_by_month, year_month):\n",
    "    '''\n",
    "    @description: \n",
    "    @param \n",
    "    @return: \n",
    "    '''\n",
    "    \n",
    "    key_words_by_month = {}\n",
    "    \n",
    "    global_index = 0\n",
    "    \n",
    "    for index, times in enumerate(times_by_month):\n",
    "        key_words_per_month = []\n",
    "        for i in range(times):\n",
    "            key_words_per_month.append(words_matrix[global_index])\n",
    "            global_index += 1\n",
    "            \n",
    "        key_words_by_month[year_month[index]] = key_words_per_month\n",
    "    \n",
    "    \n",
    "    return key_words_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 生成逐月词云\n",
    "key_words_by_month = words_separate_by_month(key_words_global, month_data['times'], month_data['year_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 2020年11月的前5条数据\n",
    "key_words_by_month['2020-11'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 提取每个月出现频率最高的关键词\n",
    "key_words_by_month_general = {}\n",
    "\n",
    "for year_month in key_words_by_month.keys():\n",
    "    key_words_per_month = []\n",
    "    for text in key_words_by_month[year_month]:\n",
    "        for word in text:\n",
    "            key_words_per_month.append(word)\n",
    "    \n",
    "#     print(key_words_per_month)\n",
    "    \n",
    "    key_words_by_month_general[year_month] = get_highest_frequency_words \\\n",
    "    (key_words_per_month, top_n=PER_MONTH_TOP_N)\n",
    "    \n",
    "    if not os.path.exists('./key_words/per_month'):\n",
    "        os.makedirs('./key_words/per_month')\n",
    "    if not os.path.exists('./key_words/per_month/Top10_keywords_'+year_month+'.txt'):\n",
    "\n",
    "        with open('./key_words/per_month/Top10_keywords_'+year_month+'.txt', 'w') as f:\n",
    "            for key_word in key_words_by_month_general[year_month]:\n",
    "                f.write(key_word+' '+str(key_words_by_month_general[year_month][key_word])+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 每个月的top10关键词\n",
    "pprint(key_words_by_month_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 生成逐月词云\n",
    "\n",
    "for year_month in key_words_by_month.keys():\n",
    "    \n",
    "    # 连接逐月关键词矩阵中所有单词\n",
    "    per_month_string = ' '.join([' '.join(x) for x in key_words_by_month[year_month]])\n",
    "\n",
    "#     print(len(per_month_string))\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud \\\n",
    "    (height=400, width=800, background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "    # 生成词云\n",
    "    wordcloud.generate(per_month_string)\n",
    "\n",
    "    if not os.path.exists('./word_cloud/per_month'):\n",
    "        os.makedirs('./word_cloud/per_month')\n",
    "    \n",
    "    if not os.path.exists('./word_cloud/per_month/wordcloud_'+year_month+'.png'):\n",
    "        wordcloud.to_file('./word_cloud/per_month/wordcloud_'+year_month+'.png')\n",
    "    \n",
    "    \n",
    "    img = Image.open('./word_cloud/per_month/wordcloud_'+year_month+'.png')\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('word cloud of %s' % year_month, fontsize='xx-large',fontweight='heavy',color='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是LDA主题模型?\n",
    "\n",
    "在LDA中，所有的文档共有同样的话题集，但是每个文档以不同的比例展示对应的话题。LDA的主要目标是自动发现一个文档集合中的话题。这些文档本身是可以观测到的，而话题的结构——话题、每个文档的话题分布和每个文档的每个词的话题赋值——是隐藏的（可称为hidden structure）。话题建模的核心计算问题就是使用观测到的文档来推断隐藏话题结构。这也可以看作是生成（generative）过程的逆过程——什么样的隐藏结构可以产生观测到的文档集合？\n",
    "\n",
    "\n",
    "### LDA模型训练\n",
    "\n",
    "默认设置有5个topic\n",
    "\n",
    "期望topic在四个象限分布越均匀越好，代表覆盖得越全面\n",
    "\n",
    "若topic之间隔的太远，则增大topic的数量\n",
    "\n",
    "若不同topic之间有较大的重叠，则减少topic的数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 读取或创建lda模型\n",
    "if not os.path.exists('./lda_model'):\n",
    "    os.makedirs('./lda_model')\n",
    "\n",
    "if os.path.exists('./lda_model/lda.model'):\n",
    "    lda_model = LdaModel.load('./lda_model/lda.model')\n",
    "else:\n",
    "\n",
    "    # 设定的主题数目\n",
    "    num_topics = NUM_TOPICS\n",
    "    iterations = ITERATIONS\n",
    "\n",
    "    # 构建LDA模型\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics\n",
    "    )\n",
    "\n",
    "    lda_model.save('./lda_model/lda.model')\n",
    "\n",
    "    \n",
    "topic_list = lda_model.print_topics(num_words=10)\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 主题分布\n",
    "topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 模型困惑度得分\n",
    "# 得分越低说明模型对文本越不困惑\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus)) # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "# 模型一致性得分\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 格式化输出所有主题构成（概率分布）\n",
    "if not os.path.exists('./lda_topics'):\n",
    "    os.makedirs('./lda_topics')\n",
    "    \n",
    "if not os.path.exists('./lda_topics/topics.txt'):\n",
    "    with open('./lda_topics/topics.txt', 'w') as f:\n",
    "        for topic in topic_list:\n",
    "            f.write('topic %d:\\n' % topic[0])\n",
    "            for word in topic[1].split('+'):\n",
    "#                 print(word)\n",
    "                f.write('  {}  {}\\n'.format(word.split('*')[1], word.split('*')[0]))\n",
    "                \n",
    "            f.write('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 每个文档都包含多个主题。但是，通常只有一个主题是主导的。\n",
    "# 下面的代码为每个句子提取该主要主题，并在格式正确的输出中显示该主题和关键字的权重。\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append\\\n",
    "                (pd.Series([int(topic_num),round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    # print(contents)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    # print(sent_topics_df)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_words)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Topic_Keywords', 'Text']\n",
    "\n",
    "\n",
    "\n",
    "df_dominant_topic.to_csv('./lda_topics/dominant_topics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 使用LDA对每篇推文的主要主题筛选\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA可视化\n",
    "- 每个主题的意义（右边的单词为某个主题下常出现的单词）\n",
    "- 每个主题在总语料库中的比重\n",
    "- 主题之间的关联\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# LDA模型可视化\n",
    "\n",
    "vis_data = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.show(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情感词典计算文本得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "SentiWordNet = pd.read_csv('./SentiWordNet3.0.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "SentiWordNet['attr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ef933335",
   "language": "python",
   "display_name": "PyCharm (TwitterDM)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}